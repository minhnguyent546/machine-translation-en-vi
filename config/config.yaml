
# general options
checkpoints_dir: checkpoints
tokenizer_basename: tokenizer_{}.json
data_loaders_basename: data_loaders.pt
model_dir: weights
model_basename: transformer
experiment_name: runs/model
src_lang: en
target_lang: vi
seed: 42

# model
d_model: 512
num_layers: 6
num_heads: 8
d_ffn: 2048
dropout_rate: 0.1
attention_dropout_rate: 0.1

# optimization
optim: adam
weight_decay: 1.0e-3
learning_rate: 1.0 # change lr to small value (e.g. 1e-4) if lr scheduler is disabled
enable_lr_scheduler: True
warmup_steps: 4000
label_smoothing: 0.1
max_grad_norm: 1.0
beam_size: 5

# dataset (huggingface)
dataset_path: mt_eng_vietnamese
dataset_subset: iwslt2015-en-vi
dataset_cache_dir: datasets
val_size: 0.025 # a rate for creating custom validation set from train set
vi_word_segmentation: True

# training
preload: latest
train_batch_size: 32
eval_batch_size: 32
num_epochs: 10
per_epoch_train_max_steps: null
val_max_steps: null
test_max_steps: null
seq_length: 128
beam_return_topk: 3
