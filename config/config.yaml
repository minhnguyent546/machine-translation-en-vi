# general options
checkpoints_dir: checkpoints
saved_checkpoints_limit: 6
tokenizer_basename: tokenizer_{}.json
model_dir: weights
model_basename: transformer
experiment_name: runs/nmt
seed: 0x3f3f3f3f

# model
d_model: 512
num_layers: 6
num_heads: 8
d_ffn: 2048
dropout: 0.2
attention_dropout: 0.2

# optimization
optim: adam
betas:
  - 0.9
  - 0.999
eps: 1.0e-8
weight_decay: 0
learning_rate: 0.5 # change lr to small value (e.g. 1e-4) if lr scheduler is disabled
enable_lr_scheduler: true
warmup_steps: 4000

# dataset
dataset_path: json
dataset_name: null
data_files:
  train: /path/to/train/file
  test: /path/to/test/file
  validation: /path/to/validation/file
dataset_save_path: /datasets/DATASET_NAME
dataset_config_kwags:
  field: data
source: source # name of the source feature
target: target # name of the target feature
tokenizer_model: bpe # possible values: word_level, bpe, word_piece
share_vocab: false # whether to share vocab between source and target sentences
source_vocab_size: 32_000 # `source_vocab_size` will be used over `target_vocab_size` when `share_vocab` is enabled
target_vocab_size: 32_000
val_size_rate: null # a rate for creating custom validation set from training set
max_set_size:
  train: 500_000
  test: 2_000
  validation: 2_000
preprocess:
  source:
    lowercase: true
    contractions: true
  target:
    lowercase: true
    vi_word_segmentation: true

# training
from_checkpoint: null # path to the pre-trained checkpoint
train_steps: 40_000
valid_interval: 3_000
save_every: 4_000
fp16: true # whether to use mixed precision during training
train_batch_size: 32
eval_batch_size: 32
src_seq_length: 120
target_seq_length: 150
label_smoothing: 0.1
max_grad_norm: 1.0
compute_bleu_kwargs:
  teacher_forcing: false
  beam_size: 4
  beam_return_topk: 3
  log_sentences: true
  logging_interval: 30
  max_steps: 300

# testing
test_checkpoint: /path/to/the/pre_trained/checkpoint

