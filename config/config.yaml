
# general options
checkpoints_dir: checkpoints
tokenizer_basename: tokenizer_{}.json
data_loaders_basename: data_loaders.pt
model_dir: weights
model_basename: transformer
experiment_name: runs/model
src_lang: en
target_lang: vi
seed: 42

# model
d_model: 512
num_layers: 6
num_heads: 8
d_ffn: 2048
dropout_rate: 0.2
attention_dropout_rate: 0.2

# optimization
optim: adam
weight_decay: 5.0e-5
learning_rate: 0.5 # change lr to small value (e.g. 1e-4) if lr scheduler is disabled
enable_lr_scheduler: True
warmup_steps: 8000
label_smoothing: 0.1
max_grad_norm: 1.0
beam_size: 5

# dataset (huggingface)
dataset_path: mt_eng_vietnamese
dataset_name: iwslt2015-en-vi
dataset_cache_dir: datasets
dataset_other_options: {}
vi_word_segmentation: True
val_size_rate: null # a rate for creating custom validation set from training set
max_set_size:
    train: 400_000
    test: 2_000
    val: 2_000

# training
preload: latest # 'latest' or epoch number
train_batch_size: 32
eval_batch_size: 32
num_epochs: 10
per_epoch_train_max_steps: null
val_max_steps: null
test_max_steps: null
seq_length: 125
beam_return_topk: 3

# testing
test_checkpoint: latest # 'latest' or epoch number
